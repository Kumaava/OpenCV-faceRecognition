{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter your id :1\n"
     ]
    }
   ],
   "source": [
    "#face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter your id :1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cam = cv2.VideoCapture(0)\n",
    "detector=cv2.CascadeClassifier('/Users/avaneeshkumar/opencv/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "Id=raw_input('enter your id :')\n",
    "sampleNum=0\n",
    "while(True):\n",
    "    ret, img = cam.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        \n",
    "        #incrementing sample number \n",
    "        sampleNum=sampleNum+1\n",
    "        #saving the captured face in the dataset folder\n",
    "        cv2.imwrite(\"dataSet/User.\"+Id +'.'+ str(sampleNum) + \".jpg\", gray[y:y+h,x:x+w])\n",
    "\n",
    "        cv2.imshow('frame',img)\n",
    "    #wait for 100 miliseconds \n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        break\n",
    "    # break if the sample number is morethan 20\n",
    "    elif sampleNum>20:\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataSet/.DS_Store', 'dataSet/User.1.1.jpg', 'dataSet/User.1.10.jpg', 'dataSet/User.1.11.jpg', 'dataSet/User.1.12.jpg', 'dataSet/User.1.13.jpg', 'dataSet/User.1.14.jpg', 'dataSet/User.1.15.jpg', 'dataSet/User.1.16.jpg', 'dataSet/User.1.17.jpg', 'dataSet/User.1.18.jpg', 'dataSet/User.1.19.jpg', 'dataSet/User.1.2.jpg', 'dataSet/User.1.20.jpg', 'dataSet/User.1.21.jpg', 'dataSet/User.1.3.jpg', 'dataSet/User.1.4.jpg', 'dataSet/User.1.5.jpg', 'dataSet/User.1.6.jpg', 'dataSet/User.1.7.jpg', 'dataSet/User.1.8.jpg', 'dataSet/User.1.9.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "recognizer = cv2.face.createLBPHFaceRecognizer()\n",
    "path = \"dataSet\" \n",
    "detector=cv2.CascadeClassifier('/Users/avaneeshkumar/opencv/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def getImagesAndLabels(path):\n",
    "    #get the path of all the files in the folder\n",
    "    imagePaths=[os.path.join(path,f) for f in os.listdir(path)]\n",
    "    print imagePaths\n",
    "    #create empth face list\n",
    "    faceSamples=[]\n",
    "    #create empty ID list\n",
    "    Ids=[]\n",
    "    #now looping through all the image paths and loading the Ids and the images\n",
    "    for imagePath in imagePaths:\n",
    "        if imagePath == \"dataSet/.DS_Store\":\n",
    "            continue\n",
    "        #loading the image and converting it to gray scale\n",
    "        pilImage=Image.open(imagePath).convert('L')\n",
    "        #Now we are converting the PIL image into numpy array\n",
    "        imageNp=np.array(pilImage,'uint8')\n",
    "        #getting the Id from the image\n",
    "        Id=int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
    "        # extract the face from the training image sample\n",
    "        faces=detector.detectMultiScale(imageNp)\n",
    "        #If a face is there then append that in the list as well as Id of it\n",
    "        for (x,y,w,h) in faces:\n",
    "            faceSamples.append(imageNp[y:y+h,x:x+w])\n",
    "            Ids.append(Id)\n",
    "    return faceSamples,Ids\n",
    "\n",
    "faces,Ids = getImagesAndLabels('dataSet')\n",
    "recognizer.train(faces, np.array(Ids))\n",
    "recognizer.save('trainner/trainner.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.2305241329\n",
      "25.8553964755\n",
      "27.5182769942\n",
      "25.0369929581\n",
      "22.18090469\n",
      "26.1319740636\n",
      "67.7554655404\n",
      "23.2240346586\n",
      "24.5902079838\n",
      "23.3497851738\n",
      "26.7498313387\n",
      "27.0238616269\n",
      "64.1086025185\n",
      "59.1463741026\n",
      "34.6693678535\n",
      "35.5427281973\n",
      "35.2986690134\n",
      "34.3576874761\n",
      "30.6962261161\n",
      "36.4123649645\n",
      "35.3064730466\n",
      "31.1882170438\n",
      "28.5528506059\n",
      "31.827209902\n",
      "33.5107999278\n",
      "31.7090962539\n",
      "59.8647006593\n",
      "33.2523843018\n",
      "33.3585462722\n",
      "33.9415850776\n",
      "27.4331095274\n",
      "26.2245031995\n",
      "28.1916055668\n",
      "26.8252388893\n",
      "27.3760620614\n",
      "27.6264595879\n",
      "26.8199371593\n",
      "29.6199082012\n",
      "30.4612553878\n",
      "28.2241274062\n",
      "29.4799391748\n",
      "30.6598454526\n",
      "30.5934054832\n",
      "32.0517609986\n",
      "30.7530677164\n",
      "31.5076454209\n",
      "30.408047461\n",
      "30.1938751193\n",
      "30.2682270501\n",
      "29.719732323\n",
      "27.3526127346\n",
      "28.499964507\n",
      "27.7226221607\n",
      "27.2785667282\n",
      "26.8064984322\n",
      "26.5865302571\n",
      "27.4036957553\n",
      "28.8870020626\n",
      "26.3100388328\n",
      "66.7070101905\n",
      "57.5300147644\n",
      "26.5457894417\n",
      "27.4869249267\n",
      "28.0854703382\n",
      "27.5913760368\n",
      "27.1324997597\n",
      "28.861303531\n",
      "31.7158900507\n",
      "33.5525147372\n",
      "30.9168455941\n",
      "28.338999484\n",
      "24.6827312302\n",
      "24.6361626472\n",
      "24.5405992871\n",
      "25.2585568125\n",
      "25.6187376994\n",
      "26.9778801931\n",
      "26.0381348316\n",
      "26.7055095052\n",
      "28.7204287865\n",
      "31.3770799808\n",
      "25.1376954959\n",
      "26.3352277605\n",
      "44.7387381343\n",
      "25.1679565135\n",
      "24.7318566533\n",
      "25.068313893\n",
      "24.7364772294\n",
      "25.5956957352\n",
      "25.5729952329\n",
      "25.3354830411\n",
      "25.7259059702\n",
      "24.4503121052\n",
      "24.6223310382\n",
      "24.0157227344\n",
      "24.6926957774\n",
      "26.1508437861\n",
      "25.9157741483\n",
      "27.8066945712\n",
      "26.7130480799\n",
      "71.9092893054\n",
      "30.0509999771\n",
      "68.5878667498\n",
      "62.7899632783\n",
      "70.0730259508\n",
      "67.040569764\n",
      "28.9733002633\n",
      "66.3611044531\n",
      "26.8558160538\n",
      "27.2865668396\n",
      "27.5237772662\n",
      "27.2908991379\n",
      "29.1627849121\n",
      "71.4033606479\n",
      "28.3348777788\n",
      "26.3728271488\n",
      "26.3930609991\n",
      "28.2256639265\n",
      "26.5537833754\n",
      "26.1694418566\n",
      "26.2606068798\n",
      "26.229409016\n",
      "26.6065316633\n",
      "26.5383430818\n",
      "26.7263379839\n",
      "27.0147488113\n",
      "27.0857985754\n",
      "27.0055356122\n",
      "29.1338942595\n",
      "26.173931316\n",
      "26.7316497553\n",
      "27.0743763261\n",
      "26.305488362\n",
      "27.0469647662\n",
      "25.9213210457\n",
      "26.726914357\n",
      "25.7066793772\n",
      "24.6718477313\n",
      "25.5893034763\n",
      "26.0575515539\n",
      "26.3921113875\n",
      "31.2682785571\n",
      "28.7530561616\n",
      "30.1189532222\n",
      "78.9167190811\n",
      "27.5499779728\n",
      "29.2986189809\n",
      "27.0850445653\n",
      "28.1969926777\n",
      "30.1783245116\n",
      "79.0548670575\n",
      "32.8244776837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0f3670a10e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'im'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "cascadePath='/Users/avaneeshkumar/opencv/data/haarcascades/haarcascade_frontalface_default.xml'\n",
    "faceCascade=cv2.CascadeClassifier(cascadePath)\n",
    "recognizer = cv2.face.createLBPHFaceRecognizer()\n",
    "recognizer.load('trainner/trainner.yml') \n",
    "id=0\n",
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, im = cam.read()\n",
    "    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    faces=faceCascade.detectMultiScale(gray, 1.2,5)\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),(225,0,0),2)\n",
    "        id, conf = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "        print conf\n",
    "        if(conf<50):\n",
    "            if(id==1):\n",
    "                id=\"Avaneesh \" + \" Age : 24 \" +  \" feeling : drowsy\"\n",
    "            elif(id==2):\n",
    "                id=\"Ghost\"\n",
    "        else:\n",
    "            id=\"Unknown\"\n",
    "        cv2.putText(im, str(id),(x,y+h),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 4)\n",
    "    cv2.imshow('im',im) \n",
    "    if (cv2.waitKey(1)==ord('q')):\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
